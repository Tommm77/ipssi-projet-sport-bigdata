from fastapi import FastAPI, HTTPException, Depends, Query, Body, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import List, Dict, Optional, Any
import httpx
import json
import os
import requests
import asyncio
from datetime import datetime, timedelta
from aiokafka import AIOKafkaProducer, AIOKafkaConsumer
from aiokafka.errors import KafkaError
import threading
import time
from aiokafka.structs import TopicPartition

# Configuration Kafka
KAFKA_BOOTSTRAP_SERVERS = "kafka:9092"
SPORTS_TOPIC = "sports_topic"
MATCHS_TOPIC = "matchs_topic"
USERS_TOPIC = "users_topic"
NOTIFICATIONS_TOPIC = "notifications_topic"

app = FastAPI(
    title="API SportData",
    description="API de donn√©es sportives pour l'application Big Data Sports",
    version="1.0.0"
)

# Configuration CORS pour permettre au frontend de communiquer
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:3000"],  # Sp√©cifier explicitement l'origine du frontend
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
    expose_headers=["*"]
)

# Mod√®les de donn√©es
class Sport(BaseModel):
    sport_id: int
    nom: str
    categorie: str
    nombre_joueurs: int
    description: str

class Match(BaseModel):
    match_id: int
    sport_id: int
    sport_nom: str
    equipe_domicile: str
    equipe_exterieur: str
    score_domicile: Optional[int] = 0
    score_exterieur: Optional[int] = 0
    date_match: str
    lieu: str
    statut: str
    derniere_mise_a_jour: Optional[str] = None

class User(BaseModel):
    user_id: int
    prenom: str
    nom: str
    email: str
    date_inscription: str
    sports_favoris: List[int]
    notification_active: bool

class Notification(BaseModel):
    notification_id: int
    user_id: int
    match_id: int
    type: str
    contenu: str
    date_envoi: str
    statut: str

class DagRun(BaseModel):
    dag_id: str
    run_id: Optional[str] = None

# Stockage en m√©moire des donn√©es pour simuler une base de donn√©es
# Ces listes seront remplies par les consommateurs Kafka
sports_data = []
matchs_data = []
users_data = []
notifications_data = []

# Producteur Kafka global
kafka_producer = None

# Initialisation du producteur Kafka
@app.on_event("startup")
async def startup_kafka_producer():
    global kafka_producer
    print("D√©marrage du backend - Initialisation des connexions Kafka...")
    
    try:
        kafka_producer = AIOKafkaProducer(
            bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS, 
            value_serializer=lambda v: json.dumps(v).encode('utf-8')
        )
        await kafka_producer.start()
        print(f"‚úÖ Producteur Kafka connect√© avec succ√®s √† {KAFKA_BOOTSTRAP_SERVERS}")
        
        # Test de connexion Kafka
        print("üîç Tentative d'envoi d'un message de test √† Kafka...")
        success = await send_to_kafka('test_topic', {"event": "backend_startup", "timestamp": str(datetime.now())})
        if success:
            print("‚úÖ Message de test envoy√© avec succ√®s √† Kafka")
        else:
            print("‚ùå √âchec de l'envoi du message de test √† Kafka")
        
        # Liste des topics √† v√©rifier
        topics_to_check = [SPORTS_TOPIC, MATCHS_TOPIC, USERS_TOPIC, NOTIFICATIONS_TOPIC]
        print(f"üîç V√©rification de l'existence des topics: {topics_to_check}")
        
        # V√©rifier l'existence des topics avec une approche diff√©rente
        try:
            # Au lieu de v√©rifier l'existence des topics, nous allons simplement essayer
            # de cr√©er un consommateur test pour chaque topic
            for topic in topics_to_check:
                print(f"üîç V√©rification du topic '{topic}'...")
                try:
                    consumer = AIOKafkaConsumer(
                        topic,
                        bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS,
                        group_id=f"check_group_{topic}",
                        auto_offset_reset="earliest"
                    )
                    await consumer.start()
                    print(f"‚úÖ Topic '{topic}' existe et est accessible")
                    await consumer.stop()
                except Exception as e:
                    print(f"‚ùå Probl√®me avec le topic '{topic}': {str(e)}")
        except Exception as e:
            print(f"‚ùå Erreur lors de la v√©rification des topics: {str(e)}")
            # Ne pas bloquer le d√©marrage de l'application en cas d'erreur de v√©rification
        
        # D√©marrer les consommateurs Kafka en arri√®re-plan avec meilleure gestion d'erreur
        print("üöÄ D√©marrage des consommateurs Kafka...")
        asyncio.create_task(run_consumer_with_error_handling(consume_sports, "sports"))
        asyncio.create_task(run_consumer_with_error_handling(consume_matchs, "matchs"))
        asyncio.create_task(run_consumer_with_error_handling(consume_users, "users"))
        asyncio.create_task(run_consumer_with_error_handling(consume_notifications, "notifications"))
        print("‚úÖ T√¢ches des consommateurs Kafka lanc√©es")
        
        # Planifier un diagnostic p√©riodique
        asyncio.create_task(diagnostic_periodique())
        
    except Exception as e:
        print(f"‚ùå Erreur lors de l'initialisation de Kafka: {str(e)}")
        print(f"‚ùå D√©tails de l'erreur: {repr(e)}")
        raise e

# Arr√™t du producteur Kafka
@app.on_event("shutdown")
async def shutdown_kafka_producer():
    global kafka_producer
    if kafka_producer is not None:
        await kafka_producer.stop()

# Fonction utilitaire pour envoyer un message √† Kafka
async def send_to_kafka(topic: str, value: Any):
    global kafka_producer
    try:
        await kafka_producer.send_and_wait(topic, value)
        return True
    except KafkaError as e:
        print(f"Erreur Kafka: {e}")
        return False

# Fonction pour g√©rer les erreurs dans les consommateurs
async def run_consumer_with_error_handling(consumer_func, consumer_name):
    try:
        print(f"üîÑ D√©marrage du consommateur {consumer_name}...")
        await consumer_func()
    except Exception as e:
        print(f"‚ùå ERREUR dans le consommateur {consumer_name}: {str(e)}")
        print(f"‚ùå D√©tails de l'erreur: {repr(e)}")
        # Apr√®s un d√©lai, tenter de red√©marrer le consommateur
        await asyncio.sleep(5)
        asyncio.create_task(run_consumer_with_error_handling(consumer_func, consumer_name))

# Fonctions des consommateurs Kafka pour chaque topic
async def consume_sports():
    try:
        # Utiliser un group_id unique avec timestamp pour forcer une relecture compl√®te
        current_time = int(time.time())
        group_id = f"sports_group_{current_time}"
        
        print(f"üîÑ Initialisation du consommateur sports avec group_id: {group_id}...")
        consumer = AIOKafkaConsumer(
            SPORTS_TOPIC,
            bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS,
            group_id=group_id,
            auto_offset_reset="earliest",  # Forcer √† lire depuis le d√©but
            value_deserializer=lambda m: json.loads(m.decode('utf-8')),
            # D√©sactiver l'auto-commit pour garantir qu'on lit tout
            enable_auto_commit=False
        )
        print(f"‚è≥ D√©marrage du consommateur sports...")
        await consumer.start()
        print(f"‚úÖ Consommateur sports d√©marr√©, en attente de messages...")
        
        try:
            # Forcer une lecture des topics depuis le d√©but
            # Assigner manuellement les partitions au d√©but
            partitions = consumer.partitions_for_topic(SPORTS_TOPIC)
            if partitions:
                print(f"üîç Partitions trouv√©es pour {SPORTS_TOPIC}: {partitions}")
                tp_list = [TopicPartition(SPORTS_TOPIC, p) for p in partitions]
                await consumer.seek_to_beginning(*tp_list)
                print("‚èÆÔ∏è Position de lecture r√©initialis√©e au d√©but pour toutes les partitions")
            
            async for msg in consumer:
                print(f"üì• Message re√ßu du topic sports: partition={msg.partition}, offset={msg.offset}")
                sport_data = msg.value
                # V√©rifier si le sport existe d√©j√† par son ID
                existing_sport = next((s for s in sports_data if s["sport_id"] == sport_data["sport_id"]), None)
                if existing_sport:
                    # Mettre √† jour le sport existant
                    idx = sports_data.index(existing_sport)
                    sports_data[idx] = sport_data
                    print(f"üîÑ Sport mis √† jour: ID={sport_data['sport_id']}, Nom={sport_data.get('nom', 'inconnu')}")
                else:
                    # Ajouter un nouveau sport
                    sports_data.append(sport_data)
                    print(f"‚ûï Nouveau sport ajout√©: ID={sport_data['sport_id']}, Nom={sport_data.get('nom', 'inconnu')}")
                
                # Committer manuellement l'offset
                await consumer.commit()
        finally:
            print("‚èπÔ∏è Arr√™t du consommateur sports...")
            await consumer.stop()
    except Exception as e:
        print(f"‚ùå Erreur dans le consommateur sports: {str(e)}")
        print(f"‚ùå D√©tails de l'erreur: {repr(e)}")
        raise

async def consume_matchs():
    try:
        # Utiliser un group_id unique avec timestamp pour forcer une relecture compl√®te
        current_time = int(time.time())
        group_id = f"matchs_group_{current_time}"
        
        print(f"üîÑ Initialisation du consommateur matchs avec group_id: {group_id}...")
        consumer = AIOKafkaConsumer(
            MATCHS_TOPIC,
            bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS,
            group_id=group_id,
            auto_offset_reset="earliest",  # Forcer √† lire depuis le d√©but
            value_deserializer=lambda m: json.loads(m.decode('utf-8')),
            # D√©sactiver l'auto-commit pour garantir qu'on lit tout
            enable_auto_commit=False
        )
        print(f"‚è≥ D√©marrage du consommateur matchs...")
        await consumer.start()
        print(f"‚úÖ Consommateur matchs d√©marr√©, en attente de messages...")
        
        try:
            # Forcer une lecture des topics depuis le d√©but
            # Assigner manuellement les partitions au d√©but
            partitions = consumer.partitions_for_topic(MATCHS_TOPIC)
            if partitions:
                print(f"üîç Partitions trouv√©es pour {MATCHS_TOPIC}: {partitions}")
                tp_list = [TopicPartition(MATCHS_TOPIC, p) for p in partitions]
                await consumer.seek_to_beginning(*tp_list)
                print("‚èÆÔ∏è Position de lecture r√©initialis√©e au d√©but pour toutes les partitions")
            
            # Dictionnaire pour stocker les matchs complets (pour g√©rer les mises √† jour partielles)
            matchs_dict = {}
            
            async for msg in consumer:
                print(f"üì• Message re√ßu du topic matchs: partition={msg.partition}, offset={msg.offset}")
                match_data = msg.value
                match_id = match_data.get("match_id")
                
                if not match_id:
                    print(f"‚ö†Ô∏è Message de match sans match_id ignor√©: {match_data}")
                    continue
                
                # V√©rifier si c'est une mise √† jour partielle ou un nouveau match complet
                is_update = "sport_id" not in match_data or "sport_nom" not in match_data
                
                # S'assurer que les scores sont toujours pr√©sents
                if "score_domicile" not in match_data:
                    match_data["score_domicile"] = 0
                if "score_exterieur" not in match_data:
                    match_data["score_exterieur"] = 0
                
                if is_update:
                    # C'est une mise √† jour partielle
                    print(f"üîÑ Mise √† jour partielle d√©tect√©e pour match_id={match_id}")
                    
                    # Chercher le match existant dans notre dictionnaire local
                    existing_match = matchs_dict.get(match_id)
                    
                    if existing_match:
                        # Mettre √† jour seulement les champs fournis
                        existing_match.update(match_data)
                        print(f"‚úÖ Match mis √† jour dans le dictionnaire local: {match_id}")
                    else:
                        # Si le match n'existe pas encore dans notre dictionnaire,
                        # on cr√©e une entr√©e de base avec les champs obligatoires par d√©faut
                        print(f"‚ö†Ô∏è Mise √† jour re√ßue pour un match inconnu: {match_id}")
                        default_match = {
                            "match_id": match_id,
                            "sport_id": 0,
                            "sport_nom": "Inconnu",
                            "equipe_domicile": "√âquipe A",
                            "equipe_exterieur": "√âquipe B",
                            "score_domicile": 0,
                            "score_exterieur": 0,
                            "date_match": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                            "lieu": "Lieu inconnu",
                            "statut": "programm√©"
                        }
                        default_match.update(match_data)
                        matchs_dict[match_id] = default_match
                        print(f"‚úÖ Nouveau match cr√©√© par d√©faut avec mise √† jour: {match_id}")
                else:
                    # C'est un nouveau match complet
                    matchs_dict[match_id] = match_data
                    print(f"‚ûï Nouveau match complet ajout√©: {match_id}")
                
                # Mettre √† jour la liste globale des matchs √† partir du dictionnaire
                global matchs_data
                matchs_data = list(matchs_dict.values())
                print(f"üìä Total des matchs en m√©moire: {len(matchs_data)}")
                
                # Committer manuellement l'offset
                await consumer.commit()
        finally:
            print("‚èπÔ∏è Arr√™t du consommateur matchs...")
            await consumer.stop()
    except Exception as e:
        print(f"‚ùå Erreur dans le consommateur matchs: {str(e)}")
        print(f"‚ùå D√©tails de l'erreur: {repr(e)}")
        raise

async def consume_users():
    try:
        # Utiliser un group_id unique avec timestamp pour forcer une relecture compl√®te
        current_time = int(time.time())
        group_id = f"users_group_{current_time}"
        
        print(f"üîÑ Initialisation du consommateur users avec group_id: {group_id}...")
        consumer = AIOKafkaConsumer(
            USERS_TOPIC,
            bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS,
            group_id=group_id,
            auto_offset_reset="earliest",  # Forcer √† lire depuis le d√©but
            value_deserializer=lambda m: json.loads(m.decode('utf-8')),
            # D√©sactiver l'auto-commit pour garantir qu'on lit tout
            enable_auto_commit=False
        )
        print(f"‚è≥ D√©marrage du consommateur users...")
        await consumer.start()
        print(f"‚úÖ Consommateur users d√©marr√©, en attente de messages...")
        
        try:
            # Forcer une lecture des topics depuis le d√©but
            # Assigner manuellement les partitions au d√©but
            partitions = consumer.partitions_for_topic(USERS_TOPIC)
            if partitions:
                print(f"üîç Partitions trouv√©es pour {USERS_TOPIC}: {partitions}")
                tp_list = [TopicPartition(USERS_TOPIC, p) for p in partitions]
                await consumer.seek_to_beginning(*tp_list)
                print("‚èÆÔ∏è Position de lecture r√©initialis√©e au d√©but pour toutes les partitions")
            
            async for msg in consumer:
                print(f"üì• Message re√ßu du topic users: partition={msg.partition}, offset={msg.offset}")
                user_data = msg.value
                # V√©rifier si l'utilisateur existe d√©j√† par son ID
                existing_user = next((u for u in users_data if u["user_id"] == user_data["user_id"]), None)
                if existing_user:
                    # Mettre √† jour l'utilisateur existant
                    idx = users_data.index(existing_user)
                    users_data[idx] = user_data
                    print(f"üîÑ Utilisateur mis √† jour: ID={user_data['user_id']}")
                else:
                    # Ajouter un nouvel utilisateur
                    users_data.append(user_data)
                    print(f"‚ûï Nouvel utilisateur ajout√©: ID={user_data['user_id']}")
                
                # Committer manuellement l'offset
                await consumer.commit()
        finally:
            print("‚èπÔ∏è Arr√™t du consommateur users...")
            await consumer.stop()
    except Exception as e:
        print(f"‚ùå Erreur dans le consommateur users: {str(e)}")
        print(f"‚ùå D√©tails de l'erreur: {repr(e)}")
        raise

async def consume_notifications():
    try:
        # Utiliser un group_id unique avec timestamp pour forcer une relecture compl√®te
        current_time = int(time.time())
        group_id = f"notifications_group_{current_time}"
        
        print(f"üîÑ Initialisation du consommateur notifications avec group_id: {group_id}...")
        consumer = AIOKafkaConsumer(
            NOTIFICATIONS_TOPIC,
            bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS,
            group_id=group_id,
            auto_offset_reset="earliest",  # Forcer √† lire depuis le d√©but
            value_deserializer=lambda m: json.loads(m.decode('utf-8')),
            # D√©sactiver l'auto-commit pour garantir qu'on lit tout
            enable_auto_commit=False
        )
        print(f"‚è≥ D√©marrage du consommateur notifications...")
        await consumer.start()
        print(f"‚úÖ Consommateur notifications d√©marr√©, en attente de messages...")
        
        try:
            # Forcer une lecture des topics depuis le d√©but
            # Assigner manuellement les partitions au d√©but
            partitions = consumer.partitions_for_topic(NOTIFICATIONS_TOPIC)
            if partitions:
                print(f"üîç Partitions trouv√©es pour {NOTIFICATIONS_TOPIC}: {partitions}")
                tp_list = [TopicPartition(NOTIFICATIONS_TOPIC, p) for p in partitions]
                await consumer.seek_to_beginning(*tp_list)
                print("‚èÆÔ∏è Position de lecture r√©initialis√©e au d√©but pour toutes les partitions")
            
            async for msg in consumer:
                print(f"üì• Message re√ßu du topic notifications: partition={msg.partition}, offset={msg.offset}")
                notification_data = msg.value
                # V√©rifier si la notification existe d√©j√† par son ID
                existing_notification = next((n for n in notifications_data if n["notification_id"] == notification_data["notification_id"]), None)
                if existing_notification:
                    # Mettre √† jour la notification existante
                    idx = notifications_data.index(existing_notification)
                    notifications_data[idx] = notification_data
                    print(f"üîÑ Notification mise √† jour: ID={notification_data['notification_id']}")
                else:
                    # Ajouter une nouvelle notification
                    notifications_data.append(notification_data)
                    print(f"‚ûï Nouvelle notification ajout√©e: ID={notification_data['notification_id']}")
                
                # Committer manuellement l'offset
                await consumer.commit()
        finally:
            print("‚èπÔ∏è Arr√™t du consommateur notifications...")
            await consumer.stop()
    except Exception as e:
        print(f"‚ùå Erreur dans le consommateur notifications: {str(e)}")
        print(f"‚ùå D√©tails de l'erreur: {repr(e)}")
        raise

# Initialiser des donn√©es de test par d√©faut
mock_sports = [
    {
        "sport_id": 1,
        "nom": "Football",
        "categorie": "Collectif",
        "nombre_joueurs": 11,
        "description": "Le football se joue avec un ballon entre deux √©quipes de 11 joueurs"
    },
    {
        "sport_id": 2,
        "nom": "Basketball",
        "categorie": "Collectif",
        "nombre_joueurs": 5,
        "description": "Le basketball est un sport o√π deux √©quipes de cinq joueurs s'affrontent pour marquer des paniers"
    }
]

mock_matchs = [
    {
        "match_id": 1001,
        "sport_id": 1,
        "sport_nom": "Football",
        "equipe_domicile": "PSG",
        "equipe_exterieur": "Marseille",
        "score_domicile": 2,
        "score_exterieur": 1,
        "date_match": "2023-11-15T20:00:00",
        "lieu": "Parc des Princes",
        "statut": "termin√©"
    }
]

# Fonction pour initialiser des donn√©es si les collections sont vides
def init_data_if_empty():
    # V√©rifier si des donn√©es r√©elles sont pr√©sentes
    # Si toutes les collections sont vides apr√®s un certain temps, on peut supposer qu'il y a un probl√®me
    # avec Kafka, et dans ce cas seulement, on charge les donn√©es fictives
    print(f"√âtat des collections - Sports: {len(sports_data)}, Matchs: {len(matchs_data)}, Users: {len(users_data)}, Notifications: {len(notifications_data)}")
    
    # Ne rien faire - laisser Kafka remplir naturellement les donn√©es
    # Les donn√©es mock ne seront plus utilis√©es automatiquement
    pass

# Endpoints API

@app.get("/")
async def root():
    return {"message": "Bienvenue sur l'API de donn√©es sportives"}

@app.get("/healthcheck")
async def healthcheck():
    services_health = {
        "api": "ok",
        "kafka": await check_kafka(),
        "hive": check_hive(),
        "airflow": check_airflow()
    }
    return services_health

async def check_kafka():
    try:
        # R√©elle v√©rification de Kafka en essayant d'envoyer un message ping
        success = await send_to_kafka('test_topic', {"ping": "healthcheck", "timestamp": str(datetime.now())})
        return "ok" if success else "down"
    except Exception as e:
        print(f"Erreur de connexion √† Kafka: {e}")
        return "down"

def check_hive():
    try:
        # En r√©alit√©, ex√©cuter une requ√™te simple sur Hive
        return "ok"
    except:
        return "down"
    
def check_airflow():
    try:
        # En r√©alit√©, v√©rifier l'API Airflow
        return "ok"
    except:
        return "down"

# Routes pour les sports
@app.get("/sports", response_model=List[Sport])
async def get_sports():
    # Ne plus initialiser avec des donn√©es fictives
    # Retourner les donn√©es de Kafka uniquement
    return sports_data

@app.get("/sports/{sport_id}", response_model=Sport)
async def get_sport(sport_id: int):
    # Chercher dans les donn√©es Kafka
    sport = next((s for s in sports_data if s["sport_id"] == sport_id), None)
    if sport is None:
        raise HTTPException(status_code=404, detail="Sport non trouv√©")
    return sport

# Cr√©er un nouveau sport et l'envoyer √† Kafka
@app.post("/sports", response_model=Sport)
async def create_sport(sport: Sport):
    # Ajouter aux donn√©es locales
    sport_dict = sport.dict()
    
    # V√©rifier si le sport existe d√©j√†
    existing_sport = next((s for s in sports_data if s["sport_id"] == sport_dict["sport_id"]), None)
    if existing_sport:
        # Mettre √† jour le sport existant
        idx = sports_data.index(existing_sport)
        sports_data[idx] = sport_dict
    else:
        # Ajouter un nouveau sport
        sports_data.append(sport_dict)
    
    # Envoyer √† Kafka
    success = await send_to_kafka(SPORTS_TOPIC, sport_dict)
    if not success:
        raise HTTPException(status_code=500, detail="Erreur lors de l'envoi des donn√©es √† Kafka")
    
    return sport

# Routes pour les matchs
@app.get("/matchs", response_model=List[Match])
async def get_matchs(
    sport_id: Optional[int] = None,
    statut: Optional[str] = None,
    date_debut: Optional[str] = None,
    date_fin: Optional[str] = None
):
    # Ne plus initialiser avec des donn√©es fictives
    # Filtrer les matchs selon les crit√®res
    matchs = matchs_data
    
    # S'assurer que chaque match a tous les champs requis
    for match in matchs:
        # Ajouter les champs manquants avec des valeurs par d√©faut
        if "score_domicile" not in match:
            match["score_domicile"] = 0
        if "score_exterieur" not in match:
            match["score_exterieur"] = 0
    
    if sport_id:
        matchs = [m for m in matchs if m["sport_id"] == sport_id]
    if statut:
        matchs = [m for m in matchs if m["statut"] == statut]
    
    # Filtres de date √† impl√©menter si n√©cessaire
    if date_debut:
        date_debut_dt = datetime.fromisoformat(date_debut)
        matchs = [m for m in matchs if datetime.fromisoformat(m["date_match"]) >= date_debut_dt]
    
    if date_fin:
        date_fin_dt = datetime.fromisoformat(date_fin)
        matchs = [m for m in matchs if datetime.fromisoformat(m["date_match"]) <= date_fin_dt]
    
    return matchs

@app.get("/matchs/{match_id}", response_model=Match)
async def get_match(match_id: int):
    match = next((m for m in matchs_data if m["match_id"] == match_id), None)
    if match is None:
        raise HTTPException(status_code=404, detail="Match non trouv√©")
    
    # S'assurer que le match a tous les champs requis
    if "score_domicile" not in match:
        match["score_domicile"] = 0
    if "score_exterieur" not in match:
        match["score_exterieur"] = 0
    
    return match

# Cr√©er un nouveau match et l'envoyer √† Kafka
@app.post("/matchs", response_model=Match)
async def create_match(match: Match):
    # Ajouter aux donn√©es locales
    match_dict = match.dict()
    
    # V√©rifier si le match existe d√©j√†
    existing_match = next((m for m in matchs_data if m["match_id"] == match_dict["match_id"]), None)
    if existing_match:
        # Mettre √† jour le match existant
        idx = matchs_data.index(existing_match)
        matchs_data[idx] = match_dict
    else:
        # Ajouter un nouveau match
        matchs_data.append(match_dict)
    
    # Envoyer √† Kafka
    success = await send_to_kafka(MATCHS_TOPIC, match_dict)
    if not success:
        raise HTTPException(status_code=500, detail="Erreur lors de l'envoi des donn√©es √† Kafka")
    
    return match

# Routes pour les statistiques
@app.get("/stats/sports")
async def get_sports_stats():
    # Calculer des statistiques √† partir des donn√©es en m√©moire
    sports_count = len(sports_data)
    
    # Compter le nombre de matchs par sport
    matchs_par_sport = {}
    for match in matchs_data:
        sport_id = match["sport_id"]
        sport_nom = match["sport_nom"]
        if sport_id not in matchs_par_sport:
            matchs_par_sport[sport_id] = {"sport_id": sport_id, "nom": sport_nom, "nombre_matchs": 0}
        matchs_par_sport[sport_id]["nombre_matchs"] += 1
    
    return list(matchs_par_sport.values())

@app.get("/stats/matchs")
async def get_matchs_stats():
    # Compter les matchs par statut
    par_statut = {
        "programm√©": 0,
        "en cours": 0,
        "termin√©": 0
    }
    
    # Compter les matchs par mois
    par_mois = {}
    
    for match in matchs_data:
        # Compter par statut
        statut = match["statut"]
        if statut in par_statut:
            par_statut[statut] += 1
        
        # Compter par mois
        date_match = datetime.fromisoformat(match["date_match"].replace("Z", "+00:00"))
        mois_cle = date_match.strftime("%Y-%m")
        if mois_cle not in par_mois:
            par_mois[mois_cle] = 0
        par_mois[mois_cle] += 1
    
    return {
        "par_statut": par_statut,
        "par_mois": par_mois
    }

# Routes pour interagir avec Airflow
@app.post("/airflow/trigger_dag")
async def trigger_dag(dag_run: DagRun):
    try:
        # Dans un environnement r√©el, appeler l'API Airflow pour d√©clencher le DAG
        print(f"D√©clenchement du DAG {dag_run.dag_id}")
        return {"status": "success", "message": f"DAG {dag_run.dag_id} d√©clench√© avec succ√®s"}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Erreur lors du d√©clenchement du DAG: {str(e)}")

@app.get("/airflow/dags")
async def get_dags():
    # Dans un environnement r√©el, r√©cup√©rer la liste des DAGs depuis Airflow
    dags = [
        {"dag_id": "sports_data_pipeline", "description": "Pipeline de traitement des donn√©es sportives", "is_active": True, "status": "success", "last_run": "2023-11-15 14:30:00"},
        {"dag_id": "g√©n√©ration_rapports", "description": "G√©n√©ration de rapports quotidiens", "is_active": True, "status": "success", "last_run": "2023-11-15 13:45:00"}
    ]
    return dags

# Route pour r√©cup√©rer les utilisateurs
@app.get("/users", response_model=List[User])
async def get_users():
    # Retourner les donn√©es r√©elles de Kafka
    return users_data

# Route pour r√©cup√©rer les notifications
@app.get("/notifications", response_model=List[Notification])
async def get_notifications(user_id: Optional[int] = None):
    # Utiliser les donn√©es r√©elles de Kafka
    notifications = notifications_data
    
    if user_id:
        notifications = [n for n in notifications if n["user_id"] == user_id]
        
    return notifications

# Cr√©er une nouvelle notification et l'envoyer √† Kafka
@app.post("/notifications", response_model=Notification)
async def create_notification(notification: Notification):
    # Envoyer √† Kafka
    success = await send_to_kafka(NOTIFICATIONS_TOPIC, notification.dict())
    if not success:
        raise HTTPException(status_code=500, detail="Erreur lors de l'envoi de la notification √† Kafka")
    
    return notification

# Fonction de diagnostic p√©riodique pour afficher l'√©tat des collections
async def diagnostic_periodique():
    try:
        while True:
            await asyncio.sleep(60)  # V√©rification toutes les minutes
            
            print("\n--- DIAGNOSTIC P√âRIODIQUE ---")
            print(f"Collections de donn√©es:")
            print(f"  - Sports: {len(sports_data)} enregistrements")
            print(f"  - Matchs: {len(matchs_data)} enregistrements")
            print(f"  - Utilisateurs: {len(users_data)} enregistrements")
            print(f"  - Notifications: {len(notifications_data)} enregistrements")
            
            # Afficher quelques exemples si disponibles
            if sports_data:
                print(f"  Exemple de sport: {sports_data[0]}")
            if matchs_data:
                print(f"  Exemple de match: {matchs_data[0]}")
            
            print("--------------------------\n")
    except Exception as e:
        print(f"‚ùå Erreur dans le diagnostic p√©riodique: {e}")
        # Red√©marrer le diagnostic apr√®s une erreur
        await asyncio.sleep(5)
        asyncio.create_task(diagnostic_periodique())

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000) 